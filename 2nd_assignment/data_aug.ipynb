{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f33c655a-995c-4931-9c6b-c7c397e8ce0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, torch, copy, cv2, sys, random\n",
    "# from datetime import datetime, timezone, timedelta\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "721f0441-4abe-40c7-b22a-c93d81f570cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시드(seed) 설정\n",
    "\n",
    "RANDOM_SEED = 2022\n",
    "\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(RANDOM_SEED)\n",
    "random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72c20f4e-ada9-4ea3-befb-50ca940235c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "\n",
    "### 데이터 디렉토리 설정 ###\n",
    "DATA_DIR= 'data'\n",
    "NUM_CLS = 2\n",
    "\n",
    "EPOCHS = 30\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 0.0005\n",
    "EARLY_STOPPING_PATIENCE = 10\n",
    "INPUT_SHAPE = 128\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc03d9f7-68dd-4c05-bdc4-c8c35ea47962",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data_dir, mode, input_shape):\n",
    "        self.data_dir = data_dir\n",
    "        self.mode = mode\n",
    "        self.input_shape = input_shape\n",
    "        \n",
    "        # Loading dataset\n",
    "        self.db = self.data_loader()\n",
    "        \n",
    "        # Dataset split\n",
    "        if self.mode == 'train':\n",
    "            self.db = self.db[:int(len(self.db) * 0.9)]\n",
    "        elif self.mode == 'val':\n",
    "            self.db = self.db[int(len(self.db) * 0.9):]\n",
    "            self.db.reset_index(inplace=True)\n",
    "        else:\n",
    "            print(f'!!! Invalid split {self.mode}... !!!')\n",
    "            \n",
    "        # Transform function\n",
    "        self.transform = transforms.Compose([transforms.Resize(self.input_shape),\n",
    "                                             transforms.ToTensor(),\n",
    "                                             transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
    "\n",
    "    def data_loader(self):\n",
    "        print('Loading ' + self.mode + ' dataset..')\n",
    "        if not os.path.isdir(self.data_dir):\n",
    "            print(f'!!! Cannot find {self.data_dir}... !!!')\n",
    "            sys.exit()\n",
    "        \n",
    "        # (COVID : 1, No : 0)\n",
    "        db = pd.read_csv(os.path.join(self.data_dir, 'train.csv'))\n",
    "        \n",
    "        return db\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.db)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data = copy.deepcopy(self.db.loc[index])\n",
    "\n",
    "        # Loading image\n",
    "        cvimg = cv2.imread(os.path.join(self.data_dir,'train',data['file_name']), cv2.IMREAD_COLOR | cv2.IMREAD_IGNORE_ORIENTATION)\n",
    "        if not isinstance(cvimg, np.ndarray):\n",
    "            raise IOError(\"Fail to read %s\" % data['file_name'])\n",
    "\n",
    "        # Preprocessing images\n",
    "        trans_image = self.transform(Image.fromarray(cvimg))\n",
    "\n",
    "        return trans_image, data['COVID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d83f43a7-58ad-4bf6-8b18-aa7556ca17aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class custom_CNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(custom_CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=8, kernel_size=5)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2)\n",
    "        self.conv2 = nn.Conv2d(in_channels=8, out_channels=25, kernel_size=5)\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features=25*29*29, out_features=128)\n",
    "        self.fc2 = nn.Linear(in_features=128, out_features=num_classes)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x))) # (32, 3, 128, 128) -> (32, 8, 62, 62)\n",
    "        x = self.pool(F.relu(self.conv2(x))) # (32, 8, 62, 62) -> (32, 25, 29, 29)\n",
    "        \n",
    "        x = torch.flatten(x,1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        \n",
    "        output = self.softmax(x)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8653990-0466-4328-9d42-36a2e65a4e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossEarlyStopper():\n",
    "    \"\"\"Early stopper\n",
    "    \n",
    "    Attributes:\n",
    "        patience (int): loss가 줄어들지 않아도 학습할 epoch 수\n",
    "        patience_counter (int): loss 가 줄어들지 않을 때 마다 1씩 증가, 감소 시 0으로 리셋\n",
    "        min_loss (float): 최소 loss\n",
    "        stop (bool): True 일 때 학습 중단\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, patience: int)-> None:\n",
    "        self.patience = patience\n",
    "\n",
    "        self.patience_counter = 0\n",
    "        self.min_loss = np.Inf\n",
    "        self.stop = False\n",
    "        self.save_model = False\n",
    "\n",
    "    def check_early_stopping(self, loss: float)-> None:\n",
    "        \"\"\"Early stopping 여부 판단\"\"\"  \n",
    "\n",
    "        if self.min_loss == np.Inf:\n",
    "            self.min_loss = loss\n",
    "            return None\n",
    "\n",
    "        elif loss > self.min_loss:\n",
    "            self.patience_counter += 1\n",
    "            msg = f\"Early stopping counter {self.patience_counter}/{self.patience}\"\n",
    "\n",
    "            if self.patience_counter == self.patience:\n",
    "                self.stop = True\n",
    "                \n",
    "        elif loss <= self.min_loss:\n",
    "            self.patience_counter = 0\n",
    "            self.save_model = True\n",
    "            msg = f\"Validation loss decreased {self.min_loss} -> {loss}\"\n",
    "            self.min_loss = loss\n",
    "        \n",
    "        print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aeffede5-a382-4cb3-90c9-bf2a6bd0b31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer():\n",
    "    \"\"\" epoch에 대한 학습 및 검증 절차 정의\"\"\"\n",
    "    \n",
    "    def __init__(self, loss_fn, model, device, metric_fn, optimizer=None, scheduler=None):\n",
    "        \"\"\" 초기화\n",
    "        \"\"\"\n",
    "        self.loss_fn = loss_fn\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.optimizer = optimizer\n",
    "        self.scheduler = scheduler\n",
    "        self.metric_fn = metric_fn\n",
    "\n",
    "    def train_epoch(self, dataloader, epoch_index):\n",
    "        \"\"\" 한 epoch에서 수행되는 학습 절차\"\"\"\n",
    "        \n",
    "        self.model.train()\n",
    "        train_total_loss = 0\n",
    "        target_lst = []\n",
    "        pred_lst = []\n",
    "        prob_lst = []\n",
    "\n",
    "        for batch_index, (img, label) in enumerate(dataloader):\n",
    "            img = img.to(self.device)\n",
    "            label = label.to(self.device).float()\n",
    "            \n",
    "            pred = self.model(img)\n",
    "            \n",
    "            loss = self.loss_fn(pred[:,1], label)\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            self.scheduler.step()\n",
    "            \n",
    "            train_total_loss += loss.item()\n",
    "            prob_lst.extend(pred[:, 1].cpu().tolist())\n",
    "            target_lst.extend(label.cpu().tolist())\n",
    "            pred_lst.extend(pred.argmax(dim=1).cpu().tolist())\n",
    "        self.train_mean_loss = train_total_loss / batch_index\n",
    "        self.train_score, f1 = self.metric_fn(y_pred=pred_lst, y_answer=target_lst)\n",
    "        msg = f'Epoch {epoch_index}, Train loss: {self.train_mean_loss}, Acc: {self.train_score}, F1-Macro: {f1}'\n",
    "        print(msg)\n",
    "\n",
    "    def validate_epoch(self, dataloader, epoch_index):\n",
    "        \"\"\" 한 epoch에서 수행되는 검증 절차\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        val_total_loss = 0\n",
    "        target_lst = []\n",
    "        pred_lst = []\n",
    "        prob_lst = []\n",
    "\n",
    "        for batch_index, (img, label) in enumerate(dataloader):\n",
    "            img = img.to(self.device)\n",
    "            label = label.to(self.device).float()\n",
    "            pred = self.model(img)\n",
    "            \n",
    "            loss = self.loss_fn(pred[:,1], label)\n",
    "            val_total_loss += loss.item()\n",
    "            prob_lst.extend(pred[:, 1].cpu().tolist())\n",
    "            target_lst.extend(label.cpu().tolist())\n",
    "            pred_lst.extend(pred.argmax(dim=1).cpu().tolist())\n",
    "        self.val_mean_loss = val_total_loss / batch_index\n",
    "        self.validation_score, f1 = self.metric_fn(y_pred=pred_lst, y_answer=target_lst)\n",
    "        msg = f'Epoch {epoch_index}, Val loss: {self.val_mean_loss}, Acc: {self.validation_score}, F1-Macro: {f1}'\n",
    "        print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97efeb0d-a175-4b21-add6-2e1764a5cbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "def get_metric_fn(y_pred, y_answer):\n",
    "    \"\"\" 성능을 반환하는 함수\"\"\"\n",
    "    \n",
    "    assert len(y_pred) == len(y_answer), 'The size of prediction and answer are not same.'\n",
    "    accuracy = accuracy_score(y_answer, y_pred)\n",
    "    f1 = f1_score(y_answer, y_pred, average='macro')\n",
    "    return accuracy, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4061a8a1-a37c-4b49-9e00-028e97f5283e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train dataset..\n",
      "Loading val dataset..\n",
      "Train set samples: 581 Val set samples: 65\n"
     ]
    }
   ],
   "source": [
    "# Load dataset & dataloader\n",
    "train_dataset = CustomDataset(data_dir=DATA_DIR, mode='train', input_shape=INPUT_SHAPE)\n",
    "validation_dataset = CustomDataset(data_dir=DATA_DIR, mode='val', input_shape=INPUT_SHAPE)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "validation_dataloader = DataLoader(validation_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "print('Train set samples:',len(train_dataset),  'Val set samples:', len(validation_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "498a91e2-97a2-4a6d-8ae5-ff24de346ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Model\n",
    "model = custom_CNN(NUM_CLS).to(DEVICE)\n",
    "\n",
    "# # Save Initial Model\n",
    "# torch.save(model.state_dict(), 'initial.pt')\n",
    "\n",
    "# Set optimizer, scheduler, loss function, metric function\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "scheduler =  optim.lr_scheduler.OneCycleLR(optimizer=optimizer, pct_start=0.1, div_factor=1e5, max_lr=0.0001, epochs=EPOCHS, steps_per_epoch=len(train_dataloader))\n",
    "loss_fn = nn.BCELoss()\n",
    "metric_fn = get_metric_fn\n",
    "\n",
    "\n",
    "# Set trainer\n",
    "trainer = Trainer(loss_fn, model, DEVICE, metric_fn, optimizer, scheduler)\n",
    "\n",
    "# Set earlystopper\n",
    "early_stopper = LossEarlyStopper(patience=EARLY_STOPPING_PATIENCE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8c7ab9fd-b3b6-4715-8bce-db2bbfad6910",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0% 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Train loss: 0.7320066326194339, Acc: 0.5851979345955249, F1-Macro: 0.562621627615677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3% 1/30 [01:34<45:52, 94.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Val loss: 1.0244255661964417, Acc: 0.5384615384615384, F1-Macro: 0.5125\n",
      "Epoch 1, Train loss: 0.7180679142475128, Acc: 0.5886402753872634, F1-Macro: 0.5884012864405022\n",
      "Epoch 1, Val loss: 0.8881474882364273, Acc: 0.6307692307692307, F1-Macro: 0.5962732919254659\n",
      "Validation loss decreased 1.0244255661964417 -> 0.8881474882364273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7% 2/30 [03:17<45:18, 97.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Train loss: 0.7049072682857513, Acc: 0.5714285714285714, F1-Macro: 0.5599087383070956\n",
      "Epoch 2, Val loss: 1.1285955309867859, Acc: 0.5846153846153846, F1-Macro: 0.5190463140586462\n",
      "Early stopping counter 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10% 3/30 [04:56<43:59, 97.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Train loss: 0.6896312601036496, Acc: 0.621342512908778, F1-Macro: 0.6212875394060063\n",
      "Epoch 3, Val loss: 0.7613501474261284, Acc: 0.6307692307692307, F1-Macro: 0.6285714285714286\n",
      "Validation loss decreased 0.8881474882364273 -> 0.7613501474261284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13% 4/30 [06:35<42:29, 98.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Train loss: 0.6587940057118734, Acc: 0.6437177280550774, F1-Macro: 0.6437177280550775\n",
      "Epoch 4, Val loss: 0.7195451259613037, Acc: 0.676923076923077, F1-Macro: 0.675694939415538\n",
      "Validation loss decreased 0.7613501474261284 -> 0.7195451259613037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17% 5/30 [08:16<41:15, 99.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Train loss: 0.6641549633608924, Acc: 0.6592082616179001, F1-Macro: 0.657502679528403\n",
      "Epoch 5, Val loss: 0.906316339969635, Acc: 0.676923076923077, F1-Macro: 0.6690909090909092\n",
      "Early stopping counter 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20% 6/30 [09:54<39:32, 98.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Train loss: 0.6229612777630488, Acc: 0.6815834767641996, F1-Macro: 0.6815797035759887\n",
      "Epoch 6, Val loss: 0.7854321002960205, Acc: 0.6461538461538462, F1-Macro: 0.6167649320687003\n",
      "Early stopping counter 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23% 7/30 [11:26<37:06, 96.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Train loss: 0.6156510992182626, Acc: 0.7108433734939759, F1-Macro: 0.7090982785751752\n",
      "Epoch 7, Val loss: 0.7753697633743286, Acc: 0.6615384615384615, F1-Macro: 0.6595238095238096\n",
      "Early stopping counter 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27% 8/30 [13:02<35:21, 96.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Train loss: 0.5741558753781848, Acc: 0.729776247848537, F1-Macro: 0.7295166307374339\n",
      "Epoch 8, Val loss: 0.6857179999351501, Acc: 0.7846153846153846, F1-Macro: 0.7845643939393939\n",
      "Validation loss decreased 0.7195451259613037 -> 0.6857179999351501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30% 9/30 [14:37<33:34, 95.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Train loss: 0.5744568490319781, Acc: 0.7418244406196214, F1-Macro: 0.7404024878467258\n",
      "Epoch 9, Val loss: 0.6752972155809402, Acc: 0.8307692307692308, F1-Macro: 0.8306088604596067\n",
      "Validation loss decreased 0.6857179999351501 -> 0.6752972155809402\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33% 10/30 [16:11<31:47, 95.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Train loss: 0.5676499572065141, Acc: 0.7228915662650602, F1-Macro: 0.7211435555754296\n",
      "Epoch 10, Val loss: 0.6901214644312859, Acc: 0.6307692307692307, F1-Macro: 0.5962732919254659\n",
      "Early stopping counter 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37% 11/30 [17:50<30:31, 96.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11, Train loss: 0.5479644619756274, Acc: 0.7573149741824441, F1-Macro: 0.755647160238265\n",
      "Epoch 11, Val loss: 0.6773225665092468, Acc: 0.7384615384615385, F1-Macro: 0.738213693437574\n",
      "Early stopping counter 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40% 12/30 [19:24<28:43, 95.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12, Train loss: 0.5250816643238068, Acc: 0.76592082616179, F1-Macro: 0.764967637540453\n",
      "Epoch 12, Val loss: 0.7691425681114197, Acc: 0.7230769230769231, F1-Macro: 0.7115384615384615\n",
      "Early stopping counter 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43% 13/30 [21:01<27:15, 96.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13, Train loss: 0.5012414885891808, Acc: 0.7831325301204819, F1-Macro: 0.7824306331581825\n",
      "Epoch 13, Val loss: 0.7222307473421097, Acc: 0.6615384615384615, F1-Macro: 0.6549227799227799\n",
      "Early stopping counter 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47% 14/30 [22:40<25:52, 97.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14, Train loss: 0.5064700378312005, Acc: 0.7796901893287436, F1-Macro: 0.7792815252748295\n",
      "Epoch 14, Val loss: 0.6739183664321899, Acc: 0.6923076923076923, F1-Macro: 0.6886973180076629\n",
      "Validation loss decreased 0.6752972155809402 -> 0.6739183664321899\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50% 15/30 [24:16<24:11, 96.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15, Train loss: 0.500286739733484, Acc: 0.7796901893287436, F1-Macro: 0.7776980653801087\n",
      "Epoch 15, Val loss: 0.8175498247146606, Acc: 0.7538461538461538, F1-Macro: 0.7523809523809524\n",
      "Early stopping counter 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53% 16/30 [25:53<22:35, 96.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16, Train loss: 0.505645043320126, Acc: 0.7831325301204819, F1-Macro: 0.7824306331581825\n",
      "Epoch 16, Val loss: 1.0814284086227417, Acc: 0.6153846153846154, F1-Macro: 0.583440143552935\n",
      "Early stopping counter 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57% 17/30 [27:29<20:54, 96.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17, Train loss: 0.5317173467742072, Acc: 0.7607573149741824, F1-Macro: 0.7588254090552087\n",
      "Epoch 17, Val loss: 0.6262766867876053, Acc: 0.7076923076923077, F1-Macro: 0.6973780936045086\n",
      "Validation loss decreased 0.6739183664321899 -> 0.6262766867876053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60% 18/30 [29:05<19:17, 96.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18, Train loss: 0.4940005871984694, Acc: 0.7762478485370051, F1-Macro: 0.7754329004329005\n",
      "Epoch 18, Val loss: 0.8391310572624207, Acc: 0.676923076923077, F1-Macro: 0.6655231560891939\n",
      "Early stopping counter 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63% 19/30 [30:32<17:09, 93.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19, Train loss: 0.4723478886816237, Acc: 0.7951807228915663, F1-Macro: 0.7941047716328615\n",
      "Epoch 19, Val loss: 0.7294589132070541, Acc: 0.7384615384615385, F1-Macro: 0.7374673319078165\n",
      "Early stopping counter 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67% 20/30 [31:44<14:31, 87.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20, Train loss: 0.4859016107188331, Acc: 0.7934595524956971, F1-Macro: 0.7924258663808503\n",
      "Epoch 20, Val loss: 1.2388448119163513, Acc: 0.7076923076923077, F1-Macro: 0.7076923076923077\n",
      "Early stopping counter 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70% 21/30 [33:03<12:40, 84.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21, Train loss: 0.4505847924285465, Acc: 0.8055077452667814, F1-Macro: 0.8043858472998138\n",
      "Epoch 21, Val loss: 0.7408526688814163, Acc: 0.6923076923076923, F1-Macro: 0.6904761904761905\n",
      "Early stopping counter 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73% 22/30 [34:39<11:43, 87.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22, Train loss: 0.4619402206606335, Acc: 0.8175559380378657, F1-Macro: 0.8167301511724795\n",
      "Epoch 22, Val loss: 0.7155401855707169, Acc: 0.7384615384615385, F1-Macro: 0.7344388368180726\n",
      "Early stopping counter 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77% 23/30 [36:16<10:34, 90.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23, Train loss: 0.4282050629456838, Acc: 0.8278829604130808, F1-Macro: 0.8263266135782099\n",
      "Epoch 23, Val loss: 0.7742991894483566, Acc: 0.7846153846153846, F1-Macro: 0.7845643939393939\n",
      "Early stopping counter 6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80% 24/30 [37:54<09:16, 92.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24, Train loss: 0.43082623928785324, Acc: 0.8037865748709122, F1-Macro: 0.8034786014384391\n",
      "Epoch 24, Val loss: 0.5784971863031387, Acc: 0.7692307692307693, F1-Macro: 0.7690120824449184\n",
      "Validation loss decreased 0.6262766867876053 -> 0.5784971863031387\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83% 25/30 [39:24<07:39, 91.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25, Train loss: 0.45767996046278214, Acc: 0.8141135972461274, F1-Macro: 0.8131832797427652\n",
      "Epoch 25, Val loss: 0.8614400625228882, Acc: 0.7538461538461538, F1-Macro: 0.7533206831119544\n",
      "Early stopping counter 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87% 26/30 [40:32<05:39, 84.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26, Train loss: 0.45562876264254254, Acc: 0.8123924268502581, F1-Macro: 0.8114068916637135\n",
      "Epoch 26, Val loss: 0.7129895836114883, Acc: 0.7230769230769231, F1-Macro: 0.7198275862068966\n",
      "Early stopping counter 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90% 27/30 [41:38<03:57, 79.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27, Train loss: 0.44910454915629494, Acc: 0.8261617900172117, F1-Macro: 0.8246470139999104\n",
      "Epoch 27, Val loss: 0.7172622829675674, Acc: 0.7692307692307693, F1-Macro: 0.7690120824449184\n",
      "Early stopping counter 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93% 28/30 [42:46<02:31, 75.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28, Train loss: 0.4375220421287749, Acc: 0.8296041308089501, F1-Macro: 0.8286212290502794\n",
      "Epoch 28, Val loss: 0.6607215106487274, Acc: 0.6923076923076923, F1-Macro: 0.6904761904761905\n",
      "Early stopping counter 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97% 29/30 [43:48<01:11, 71.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29, Train loss: 0.4514554755555259, Acc: 0.8382099827882961, F1-Macro: 0.8368523563712837\n",
      "Epoch 29, Val loss: 0.8458727598190308, Acc: 0.7538461538461538, F1-Macro: 0.7523809523809524\n",
      "Early stopping counter 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 30/30 [44:51<00:00, 89.73s/it]\n"
     ]
    }
   ],
   "source": [
    "for epoch_index in tqdm(range(EPOCHS)):\n",
    "\n",
    "    trainer.train_epoch(train_dataloader, epoch_index)\n",
    "    trainer.validate_epoch(validation_dataloader, epoch_index)\n",
    "\n",
    "    # early_stopping check\n",
    "    early_stopper.check_early_stopping(loss=trainer.val_mean_loss)\n",
    "\n",
    "    if early_stopper.stop:\n",
    "        print('Early stopped')\n",
    "        break\n",
    "\n",
    "    if early_stopper.save_model:\n",
    "        check_point = {\n",
    "            'model': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'scheduler': scheduler.state_dict()\n",
    "        }\n",
    "        torch.save(check_point, 'best.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ddf831-c3b3-4c42-817f-769a6c503f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINED_MODEL_PATH = 'best.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23cf6a5-8bcc-4a76-a437-171dd552e27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    def __init__(self, data_dir, input_shape):\n",
    "        self.data_dir = data_dir\n",
    "        self.input_shape = input_shape\n",
    "        \n",
    "        # Loading dataset\n",
    "        self.db = self.data_loader()\n",
    "        \n",
    "        # Transform function\n",
    "        self.transform = transforms.Compose([transforms.Resize(self.input_shape),\n",
    "                                             \n",
    "                                             transforms.ToTensor(),\n",
    "                                             transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
    "\n",
    "    def data_loader(self):\n",
    "        print('Loading test dataset..')\n",
    "        if not os.path.isdir(self.data_dir):\n",
    "            print(f'!!! Cannot find {self.data_dir}... !!!')\n",
    "            sys.exit()\n",
    "        \n",
    "        db = pd.read_csv(os.path.join(self.data_dir, 'sample_submission.csv'))\n",
    "        return db\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.db)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        data = copy.deepcopy(self.db.loc[index])\n",
    "        \n",
    "        # Loading image\n",
    "        cvimg = cv2.imread(os.path.join(self.data_dir,'test',data['file_name']), cv2.IMREAD_COLOR | cv2.IMREAD_IGNORE_ORIENTATION)\n",
    "        if not isinstance(cvimg, np.ndarray):\n",
    "            raise IOError(\"Fail to read %s\" % data['file_name'])\n",
    "\n",
    "        # Preprocessing images\n",
    "        trans_image = self.transform(Image.fromarray(cvimg))\n",
    "\n",
    "        return trans_image, data['file_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b431f5f1-0fb0-45c0-9afc-4a7957743165",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
