{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68696f7d-99f7-4a1b-97b1-882d0c5565e9",
   "metadata": {},
   "source": [
    "# [모의 캐글 - 게임] 비매너 댓글 식별 \n",
    "\n",
    "- 자연어 multi label classification 과제\n",
    "- 작성자 : MNC Sukyung Kim (skkim@mnc.ai)\n",
    "\n",
    "참고 논문 : \n",
    "- [BERT: Pre-training of Deep Bidirectional Transformers for\n",
    "Language Understanding](https://arxiv.org/pdf/1810.04805.pdf)\n",
    "- [Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bfa42a1-848b-4d3b-b13e-b65cf0972404",
   "metadata": {},
   "source": [
    "# 1. 환경 설정 및 라이브러리 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c0e90b-6f4c-45fd-b83c-140dcde11d2b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff15bef2-c669-4cb1-bf76-256154c4858b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import shutil\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from datetime import datetime, timezone, timedelta\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "from attrdict import AttrDict\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils import *\n",
    "from torch.optim import Adam, AdamW\n",
    "\n",
    "from transformers import logging, get_linear_schedule_with_warmup\n",
    "\n",
    "\n",
    "from transformers import ( \n",
    "    BertConfig,\n",
    "    ElectraConfig,\n",
    "    ElectraConfig\n",
    ")\n",
    "\n",
    "from transformers import (\n",
    "    BertTokenizer,  \n",
    "    AutoTokenizer,\n",
    "    ElectraTokenizer,\n",
    ")\n",
    "\n",
    "from transformers import (\n",
    "    BertModel,\n",
    "    AutoModel, \n",
    "    ElectraForSequenceClassification,\n",
    "    BertForSequenceClassification\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c95b317-b37f-4c59-8eaf-25ce048eb753",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사용할 GPU 지정\n",
    "print(\"number of GPUs: \", torch.cuda.device_count())\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print(\"Does GPU exist? : \", use_cuda)\n",
    "DEVICE = torch.device(\"cuda\" if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ffbcad1-d170-462f-807d-41760e65f4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# True 일 때 코드를 실행하면 example 등을 보여줌\n",
    "DEBUG = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7484847-924d-4f99-8b41-190ae4f192e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config 파일 불러오기\n",
    "config_path = os.path.join('config.json')\n",
    "\n",
    "def set_config(config_path):\n",
    "    if os.path.lexists(config_path):\n",
    "        with open(config_path) as f:\n",
    "            args = AttrDict(json.load(f))\n",
    "            print(\"config file loaded.\")\n",
    "            print(args.pretrained_model)\n",
    "    else:\n",
    "        assert False, 'config json file cannot be found.. please check the path again.'\n",
    "    \n",
    "    return args\n",
    "    \n",
    "\n",
    "# 코드 중간중간에 끼워넣어 리셋 가능\n",
    "args = set_config(config_path)\n",
    "\n",
    "# 결과 저장 폴더 미리 생성\n",
    "os.makedirs(args.result_dir, exist_ok=True)\n",
    "os.makedirs(args.config_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a04c99c-208b-46a1-82cd-a7c9c776c8ad",
   "metadata": {},
   "source": [
    "# 2. EDA 및 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87df6631-40e2-4c33-bacc-6548f9459d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data 경로 설정  \n",
    "train_path = os.path.join(args.data_dir,'train.csv')\n",
    "\n",
    "print(\"train 데이터 경로가 올바른가요? : \", os.path.lexists(train_path))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3123598-d707-467a-a380-99644a9a3637",
   "metadata": {},
   "source": [
    "### 2-1. Train 데이터 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d5cb19-0e61-4358-9a9b-c690cbe8c6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(train_path, encoding = 'UTF-8-SIG')\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10dee23-db6d-4fce-8b06-054c3457adaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ecb32ed-e7f9-4640-89c2-87005a9aed31",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"bias classes: \", train_df.bias.unique())\n",
    "print(\"hate classes: \", train_df.hate.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faecbd1a-82ee-40ed-b81a-2984674b669f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(train_df.bias, train_df.hate, margins=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d3802b-a501-4adc-9f9a-6968df7684d3",
   "metadata": {},
   "source": [
    "### 2-2. Test 데이터 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86211318-bfaa-47c7-9653-63a13905dc51",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = os.path.join(args.data_dir,'test.csv')\n",
    "print(\"test 데이터 경로가 올바른가요? : \", os.path.lexists(test_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a713f829-09c9-468a-8266-4bd555e29906",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(test_path)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad5fbb7-acb5-47cb-b0f3-b220bf78ab67",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44bb5aa8-6284-4066-a69d-a1539c5287a5",
   "metadata": {},
   "source": [
    "### 2-3. 데이터 전처리 (Label Encoding)\n",
    "bias, hate 라벨들의 class를 정수로 변경하여 라벨 인코딩을 하기 위한 딕셔너리입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11018420-84da-40ac-b892-4e47f20dd83f",
   "metadata": {},
   "source": [
    "- bias, hate 컬럼을 합쳐서 하나의 라벨로 만들기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c61bc4-da4a-4fc3-9cff-62079051ed82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 두 라벨의 가능한 모든 조합 만들기\n",
    "combinations = np.array(np.meshgrid(train_df.bias.unique(), train_df.hate.unique())).T.reshape(-1,2)\n",
    "\n",
    "if DEBUG==True:\n",
    "    print(combinations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1f00cb-8009-460f-8903-ca876d647ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bias, hate 컬럼을 합친 것\n",
    "bias_hate = list(np.array([train_df['bias'].values, train_df['hate'].values]).T.reshape(-1,2))\n",
    "\n",
    "if DEBUG==True:\n",
    "    print(bias_hate[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26376de0-2a54-44c2-a8af-f20025b9798b",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "for i, arr in enumerate(bias_hate):\n",
    "    for idx, elem in enumerate(combinations):\n",
    "        if np.array_equal(elem, arr):\n",
    "            labels.append(idx)\n",
    "\n",
    "train_df['label'] = labels\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add80824-55c5-4ca3-9858-5aeee3ede98d",
   "metadata": {},
   "source": [
    "## 3. Dataset 로드"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9956c57-af6a-4330-a531-a338be9da9c4",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3-0. Pre-trained tokenizer 탐색"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6f75f4-cdf7-4998-8b78-3aa716b7045b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config.json 에서 지정 이름별로 가져올 라이브러리 지정\n",
    "\n",
    "TOKENIZER_CLASSES = {\n",
    "    \"BertTokenizer\": BertTokenizer\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a285fec-0b39-4dce-8dc0-6faa6b027c34",
   "metadata": {
    "tags": []
   },
   "source": [
    "- Tokenizer 사용 예시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0f1e57-caf8-4d67-a2a9-61284a793f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKENIZER = TOKENIZER_CLASSES[args.tokenizer_class].from_pretrained(args.pretrained_model)\n",
    "if DEBUG==True:\n",
    "    print(TOKENIZER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8cdb8e-41a8-4d51-8aa0-f305782416ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBUG == True:\n",
    "    example = train_df['title'][0]\n",
    "    print(TOKENIZER(example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f174365c-90da-4c32-b6b4-5d66525a0687",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if DEBUG==True:\n",
    "    print(TOKENIZER.encode(example),\"\\n\")\n",
    "    \n",
    "    # 토큰으로 나누기\n",
    "    print(TOKENIZER.tokenize(example),\"\\n\")\n",
    "    \n",
    "    # 토큰 id로 매핑하기\n",
    "    print(TOKENIZER.convert_tokens_to_ids(TOKENIZER.tokenize(example)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf1708e-aedb-402f-a9c3-20ebdbe537eb",
   "metadata": {},
   "source": [
    "### 3-1. Dataset 만드는 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e6f065-671c-435c-8e73-8cad3c1496a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, df, tokenizer, max_len, mode = 'train'):\n",
    "\n",
    "        self.data = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.mode = mode\n",
    "        \n",
    "        if self.mode!='test':\n",
    "            try: \n",
    "                self.labels = df['label'].tolist()\n",
    "            except:\n",
    "                assert False, 'CustomDataset Error : \\'label\\' column does not exist in the dataframe'\n",
    "     \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "                \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        전체 데이터에서 특정 인덱스 (idx)에 해당하는 기사제목과 댓글 내용을 \n",
    "        토크나이즈한 data('input_ids', 'attention_mask','token_type_ids')의 딕셔너리 형태로 불러옴\n",
    "        \"\"\"\n",
    "        title = self.data.title.iloc[idx]\n",
    "        comment = self.data.comment.iloc[idx]\n",
    "        \n",
    "        tokenized_text = self.tokenizer(title, comment,\n",
    "                             padding= 'max_length',\n",
    "                             max_length=self.max_len,\n",
    "                             truncation=True,\n",
    "                             return_token_type_ids=True,\n",
    "                             return_attention_mask=True,\n",
    "                             return_tensors = \"pt\")\n",
    "        \n",
    "        data = {'input_ids': tokenized_text['input_ids'].clone().detach().long(),\n",
    "               'attention_mask': tokenized_text['attention_mask'].clone().detach().long(),\n",
    "               'token_type_ids': tokenized_text['token_type_ids'].clone().detach().long(),\n",
    "               }\n",
    "        \n",
    "        if self.mode != 'test':\n",
    "            label = self.data.label.iloc[idx]\n",
    "            return data, label\n",
    "        else:\n",
    "            return data\n",
    "        \n",
    "\n",
    "    \n",
    "train_dataset = CustomDataset(train_df, TOKENIZER, args.max_seq_len, mode ='train')\n",
    "print(\"train dataset loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d27241c-4c3b-498e-9456-871dc3cc2e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBUG ==True :\n",
    "    print(\"dataset sample : \")\n",
    "    print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbadd376-0df2-455c-89f1-9236ba9d32a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoded_plus = tokenizer.encode_plus(\n",
    "#                     sentence,                      # Sentence to encode.\n",
    "#                     add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "#                     max_length = 128,           # Pad & truncate all sentences.\n",
    "#                     pad_to_max_length = True,\n",
    "#                     return_attention_mask = True,   # Construct attention masks.\n",
    "#                     return_tensors = 'pt',     # Return pytorch tensors.\n",
    "#                )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e92d12dd-7a1e-423f-8b3d-00bae3c17375",
   "metadata": {},
   "source": [
    "### 3-2. Train, Validation set 나누기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6fb9525-861d-48ba-b495-828b8a08b9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "                                                         \n",
    "train_data, val_data = train_test_split(train_df, test_size=0.1, random_state=args.seed)\n",
    "\n",
    "train_dataset = CustomDataset(train_data, TOKENIZER, args.max_seq_len, 'train')\n",
    "val_dataset = CustomDataset(val_data, TOKENIZER, args.max_seq_len, 'validation')\n",
    "\n",
    "print(\"Train dataset: \", len(train_dataset))\n",
    "print(\"Validation dataset: \", len(val_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed493727-edd8-43ba-924f-195837306952",
   "metadata": {},
   "source": [
    "## 4. 분류 모델 학습을 위한 세팅"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c0817a-8b51-4d22-9f31-eb8bd22640cb",
   "metadata": {},
   "source": [
    "### 4-1. BertForSequenceClassification 설정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a250764-25aa-43e4-a383-f464dc54bc25",
   "metadata": {},
   "source": [
    "\n",
    "(https://huggingface.co/docs/transformers/v4.16.2/en/main_classes/configuration#transformers.PretrainedConfig.from_pretrained)\n",
    "\n",
    "[PretrainedConfig](https://huggingface.co/transformers/v3.0.2/main_classes/configuration.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554c0acd-d158-4d36-bfd1-abb47fb618f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import logging\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "# config.json 에 입력된 architecture 에 따라 베이스 모델 설정\n",
    "BASE_MODELS = {\n",
    "    \"BertForSequenceClassification\": BertForSequenceClassification\n",
    "}\n",
    "\n",
    "\n",
    "myModel = BASE_MODELS[args.architecture].from_pretrained(args.pretrained_model, \n",
    "                                                         num_labels = args.num_classes, \n",
    "                                                         output_attentions = False, # Whether the model returns attentions weights.\n",
    "                                                         output_hidden_states = True # Whether the model returns all hidden-states.\n",
    "                                                        )\n",
    "if DEBUG==True:\n",
    "    # 모델 구조 확인\n",
    "    print(myModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f90f3c-51f6-4b44-a426-a0b36ccfa852",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install git+https://git@github.com/SKTBrain/KoBERT.git@master"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c022a093-130f-40ac-b8e5-8decc5c63a6d",
   "metadata": {},
   "source": [
    "### 4-2. 모델 설정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d37b1b-d491-4d3c-8551-980f78a78337",
   "metadata": {},
   "source": [
    "\n",
    "BertForSequenceClassifier (line 1232부터 참고) [source code](https://github.com/huggingface/transformers/blob/a39dfe4fb122c11be98a563fb8ca43b322e01036/src/transformers/modeling_bert.py#L1284-L1287)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1685ce12-7238-4e2a-a20d-11969b568807",
   "metadata": {},
   "outputs": [],
   "source": [
    "class myClassifier(nn.Module):\n",
    "    def __init__(self, model, hidden_size = 768, num_classes=args.num_classes, dr_rate=None, params=None):\n",
    "        super(myClassifier, self).__init__()\n",
    "        self.model = model\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, token_ids, attention_mask, segment_ids):      \n",
    "        outputs = self.model(input_ids = token_ids, \n",
    "                             token_type_ids = segment_ids.long(), \n",
    "                             attention_mask = attention_mask.float().to(token_ids.device))\n",
    "         \n",
    "        logits = outputs.logits\n",
    "        output = self.softmax(logits)\n",
    "        return output\n",
    "        \n",
    "model = myClassifier(myModel, dr_rate=0.1)\n",
    "\n",
    "# if DEBUG ==True :\n",
    "#     print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99386a8c-7795-454c-9a32-eaf447c6fc4c",
   "metadata": {},
   "source": [
    "### 4-3. 모델 구성 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b6bc6f-1df3-46fb-a607-5de3e3cef571",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = list(model.named_parameters())\n",
    "\n",
    "print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
    "\n",
    "print('==== Embedding Layer ====\\n')\n",
    "\n",
    "for p in params[0:5]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== First Transformer ====\\n')\n",
    "\n",
    "for p in params[5:21]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== Output Layer ====\\n')\n",
    "\n",
    "for p in params[-4:]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c441a1-2ce3-46b2-b52d-c0cce7f76873",
   "metadata": {},
   "source": [
    "## 5. 학습 진행"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4643714-c372-463a-8f7e-2955b2d4376a",
   "metadata": {},
   "source": [
    "### 5-0. Early Stopper 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c205350d-ce58-42e7-9558-2b405f142ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossEarlyStopper():\n",
    "    \"\"\"Early stopper\n",
    "\n",
    "        patience (int): loss가 줄어들지 않아도 학습할 epoch 수\n",
    "        patience_counter (int): loss 가 줄어들지 않을 때 마다 1씩 증가\n",
    "        min_loss (float): 최소 loss\n",
    "        stop (bool): True 일 때 학습 중단\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, patience: int)-> None:\n",
    "        \"\"\" 초기화\n",
    "\n",
    "        Args:\n",
    "            patience (int): loss가 줄어들지 않아도 학습할 epoch 수\n",
    "            weight_path (str): weight 저장경로\n",
    "            verbose (bool): 로그 출력 여부, True 일 때 로그 출력\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.patience_counter = 0\n",
    "        self.min_loss = np.Inf\n",
    "        self.stop = False\n",
    "\n",
    "    def check_early_stopping(self, loss: float)-> None:\n",
    "        msg = ''\n",
    "        # 첫 에폭\n",
    "        if self.min_loss == np.Inf:\n",
    "            self.min_loss = loss\n",
    "           \n",
    "        # loss가 줄지 않는다면 -> patience_counter 1 증가\n",
    "        elif loss > self.min_loss:\n",
    "            self.patience_counter += 1\n",
    "            msg = f\"Early stopping counter {self.patience_counter}/{self.patience}\"\n",
    "\n",
    "            # patience 만큼 loss가 줄지 않았다면 학습을 중단합니다.\n",
    "            if self.patience_counter == self.patience:\n",
    "                self.stop = True\n",
    "  \n",
    "        # loss가 줄어듬 -> min_loss 갱신, patience_counter 초기화\n",
    "        elif loss <= self.min_loss:\n",
    "            self.patience_counter = 0\n",
    "            self.save_model = True\n",
    "            msg = f\"Validation loss decreased {self.min_loss} -> {loss}\"\n",
    "            self.min_loss = loss\n",
    "\n",
    "        print(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d11163-6def-448a-9095-19231e9efe64",
   "metadata": {},
   "source": [
    "### 5-1. Epoch 별 학습 및 검증"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06ed2a1-c2e3-43cb-9967-05d85b8546ee",
   "metadata": {},
   "source": [
    "- Adam optimizer의 epsilon 파라미터 eps = 1e-8 는 \"계산 중 0으로 나눔을 방지 하기 위한 아주 작은 숫자 \" 입니다. ([출처](https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/))\n",
    "- `warmup_ratio` : \n",
    "  - 학습이 진행되면서 학습률을 그 상황에 맞게 가변적으로 적당하게 변경되게 하기 위해 Scheduler를 사용합니다.\n",
    "  - 처음 학습률(Learning rate)를 warm up하기 위한 비율을 설정하는 warmup_ratio을 설정합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d151484e-978d-4bf3-b7b3-fe2c5ba5eda5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# args = set_config(config_path)\n",
    "\n",
    "logging.set_verbosity_warning()\n",
    "\n",
    "# 재현을 위해 모든 곳의 시드 고정\n",
    "seed_val = args.seed\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "def train(model, train_data, val_data, args, mode = 'train'):\n",
    "    \n",
    "    # args.run은 실험 이름 (어디까지나 팀원들간의 버전 관리 및 공유 편의를 위한 것으로, 자유롭게 수정 가능합니다.)\n",
    "    print(\"RUN : \", args.run)\n",
    "    shutil.copyfile(\"config.json\", os.path.join(args.config_dir, f\"config_{args.run}.json\"))\n",
    "\n",
    "    early_stopper = LossEarlyStopper(patience=args.patience)\n",
    "    \n",
    "    train_dataloader = torch.utils.data.DataLoader(train_data, batch_size=args.train_batch_size, shuffle=True)\n",
    "    val_dataloader = torch.utils.data.DataLoader(val_data, batch_size=args.train_batch_size)\n",
    "\n",
    "    DEBUG=False\n",
    "    \n",
    "    if DEBUG == True:\n",
    "        # 데이터로더가 성공적으로 로드 되었는지 확인\n",
    "        for idx, data in enumerate(train_dataloader):\n",
    "            if idx==0:\n",
    "                print(\"batch size : \", len(data[0]['input_ids']))\n",
    "                print(\"The first batch looks like ..\\n\", data[0])\n",
    "    \n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    total_steps = len(train_dataloader) * args.train_epochs\n",
    "\n",
    "    optimizer = Adam(model.parameters(), lr=args.learning_rate, eps=args.adam_epsilon)\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=int(total_steps * args.warmup_proportion), num_training_steps=total_steps)\n",
    "\n",
    "    \n",
    "    if use_cuda:\n",
    "        model = model.to(DEVICE)\n",
    "        criterion = criterion.to(DEVICE)\n",
    "        \n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    tr_loss = 0.0\n",
    "    val_loss = 0\n",
    "    best_score = 0\n",
    "      \n",
    "\n",
    "    for epoch_num in range(args.train_epochs):\n",
    "\n",
    "            total_acc_train = 0\n",
    "            total_loss_train = 0\n",
    "            \n",
    "            assert mode in ['train', 'val'], 'your mode should be either \\'train\\' or \\'val\\''\n",
    "            \n",
    "            if mode =='train':\n",
    "                for train_input, train_label in tqdm(train_dataloader):\n",
    "                    \n",
    "                    mask = train_input['attention_mask'].to(DEVICE)\n",
    "                    input_id = train_input['input_ids'].squeeze(1).to(DEVICE)\n",
    "                    segment_ids = train_input['token_type_ids'].squeeze(1).to(DEVICE)\n",
    "                    train_label = train_label.long().to(DEVICE)                \n",
    "\n",
    "                    output = model(input_id, mask, segment_ids)\n",
    "                    \n",
    "                    batch_loss = criterion(output.view(-1,6), train_label.view(-1))\n",
    "                    total_loss_train += batch_loss.item()\n",
    "\n",
    "                    acc = (output.argmax(dim=1) == train_label).sum().item()\n",
    "                    total_acc_train += acc\n",
    "\n",
    "                    model.zero_grad()\n",
    "                    batch_loss.backward()\n",
    "                    optimizer.step()\n",
    "                    \n",
    "\n",
    "            total_acc_val = 0\n",
    "            total_loss_val = 0\n",
    "            \n",
    "            # validation을 위해 이걸 넣으면 이 evaluation 프로세스 중엔 dropout 레이어가 다르가 동작한다.\n",
    "            model.eval()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "\n",
    "                for val_input, val_label in val_dataloader:\n",
    "\n",
    "                    mask = val_input['attention_mask'].to(DEVICE)\n",
    "                    input_id = val_input['input_ids'].squeeze(1).to(DEVICE)\n",
    "                    segment_ids = val_input['token_type_ids'].squeeze(1).to(DEVICE)\n",
    "                    val_label = val_label.long().to(DEVICE)\n",
    "\n",
    "                    output = model(input_id, mask, segment_ids)\n",
    "\n",
    "                    batch_loss = criterion(output.view(-1,6), val_label.view(-1))\n",
    "                    total_loss_val += batch_loss.item()\n",
    "                    \n",
    "                    acc = (output.argmax(dim=1) == val_label).sum().item()\n",
    "                    total_acc_val += acc\n",
    "            \n",
    "            \n",
    "            train_loss = total_loss_train / len(train_data)\n",
    "            train_accuracy = total_acc_train / len(train_data)\n",
    "            val_loss = total_loss_val / len(val_data)\n",
    "            val_accuracy = total_acc_val / len(val_data)\n",
    "            \n",
    "            # 한 Epoch 학습 후 학습/검증에 대해 loss와 평가지표 (여기서는 accuracy로 임의로 설정) 출력\n",
    "            print(\n",
    "                f'Epoch: {epoch_num + 1} \\\n",
    "                | Train Loss: {train_loss: .3f} \\\n",
    "                | Train Accuracy: {train_accuracy: .3f} \\\n",
    "                | Val Loss: {val_loss: .3f} \\\n",
    "                | Val Accuracy: {val_accuracy: .3f}')\n",
    "          \n",
    "            # early_stopping check\n",
    "            early_stopper.check_early_stopping(loss=val_loss)\n",
    "\n",
    "            if early_stopper.stop:\n",
    "                print('Early stopped, Best score : ', best_score)\n",
    "                break\n",
    "\n",
    "            if val_accuracy > best_score:\n",
    "            # 모델이 개선됨 -> 검증 점수와 weight 갱신\n",
    "                best_score = val_accuracy\n",
    "                \n",
    "                # 학습된 모델을 저장할 디렉토리 및 모델 이름 지정\n",
    "                SAVED_MODEL =  os.path.join(args.result_dir, f'best_{args.run}.pt')\n",
    "            \n",
    "                check_point = {\n",
    "                    'model': model.state_dict(),\n",
    "                    'optimizer': optimizer.state_dict(),\n",
    "                    'scheduler': scheduler.state_dict()\n",
    "                }\n",
    "                torch.save(check_point, SAVED_MODEL)  \n",
    "              \n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "train(model, train_dataset, val_dataset, args, mode = 'train')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6301514c-7d9e-4f37-9e8c-db85e3819b8b",
   "metadata": {},
   "source": [
    "## 6. Test dataset으로 추론 (Prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32696c34-b5e6-43b3-a1e7-ed8c2847baa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# 테스트 데이터셋 불러오기\n",
    "test_data = CustomDataset(test_df, tokenizer = TOKENIZER, max_len= args.max_seq_len, mode='test')\n",
    "\n",
    "def test(model, SAVED_MODEL, test_data, args, mode = 'test'):\n",
    "\n",
    "\n",
    "    test_dataloader = torch.utils.data.DataLoader(test_data, batch_size=args.eval_batch_size)\n",
    "\n",
    "\n",
    "    if use_cuda:\n",
    "\n",
    "        model = model.to(DEVICE)\n",
    "        model.load_state_dict(torch.load(SAVED_MODEL)['model'])\n",
    "\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    pred = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for test_input in test_dataloader:\n",
    "\n",
    "            mask = test_input['attention_mask'].to(DEVICE)\n",
    "            input_id = test_input['input_ids'].squeeze(1).to(DEVICE)\n",
    "            segment_ids = test_input['token_type_ids'].squeeze(1).to(DEVICE)\n",
    "\n",
    "            output = model(input_id, mask, segment_ids)\n",
    "\n",
    "            output = output.argmax(dim=1).cpu().tolist()\n",
    "\n",
    "            for label in output:\n",
    "                pred.append(label)\n",
    "                \n",
    "    return pred\n",
    "\n",
    "SAVED_MODEL =  os.path.join(args.result_dir, f'best_{args.run}.pt')\n",
    "\n",
    "pred = test(model, SAVED_MODEL, test_data, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29432647-7a5d-4dda-99af-ad6f151fd2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"prediction completed for \", len(pred), \"comments\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8aaf8d-93c6-4998-b005-f32ec5986f46",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ccc3c34-e3a8-4172-8b16-f1c2a5bb5a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0-5 사이의 라벨 값 별로 bias, hate로 디코딩 하기 위한 딕셔너리\n",
    "bias_dict = {0: 'none', 1: 'none', 2: 'others', 3:'others', 4:'gender', 5:'gender'}\n",
    "hate_dict = {0: 'none', 1: 'hate', 2: 'none', 3:'hate', 4:'none', 5:'hate'}\n",
    "\n",
    "# 인코딩 값으로 나온 타겟 변수를 디코딩\n",
    "pred_bias = ['' for i in range(len(pred))]\n",
    "pred_hate = ['' for i in range(len(pred))]\n",
    "\n",
    "for idx, label in enumerate(pred):\n",
    "    pred_bias[idx]=(str(bias_dict[label]))\n",
    "    pred_hate[idx]=(str(hate_dict[label]))\n",
    "print('decode Completed!')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e6cdb3-8245-468e-aaf0-4e6f328e98c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "submit = pd.read_csv(os.path.join(args.data_dir,'sample_submission.csv'))\n",
    "\n",
    "submit['bias'] = pred_bias\n",
    "submit['hate'] = pred_hate\n",
    "submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964b5159-8ffe-4cc2-833b-3c4b0c857022",
   "metadata": {},
   "outputs": [],
   "source": [
    "submit.to_csv(os.path.join(args.result_dir, f\"submission_{args.run}.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a30767-7803-448a-950b-412b71088d07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89023af-867b-4d28-b30d-e465d0abe66a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
